<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Konstantin Mishchenko on Konstantin Mishchenko</title>
    <link>https://konstmish.github.io/</link>
    <description>Recent content in Konstantin Mishchenko on Konstantin Mishchenko</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Konstantin Mishchenko</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>My sessions at ICCCOPT</title>
      <link>https://konstmish.github.io/post/2019_iccopt/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/post/2019_iccopt/</guid>
      <description>&lt;p&gt;I was at the ICCOPT conference in Berlin from 5 to 8 August as the chair of 3 sessions: 2 on variational inequality/minimax/GANs and 1 on non-smooth optimization.&lt;/p&gt;

&lt;p&gt;ICCOPT is my favorite conference so far. It was completely devoted to optimization and the density of interesting talks was overwhelming, very often I could not decide to which parallel session I should go. I was lucky to meet with many brilliant people, whose work inspired some of my papers. I find the diversity of the presented topics quite high, and it is surprising how many different ideas and applications are covered by mathematical optimization.&lt;/p&gt;

&lt;p&gt;Organizing was quite fun as well. Next time I may try to organize a couple more sessions to see if it is even more fun at a larger scale.&lt;/p&gt;

&lt;p&gt;My 3 sessions were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Variational Inequalities, Minimax Problems and GANs (Part 1, Monday). Speakers: Simon Lacoste-Julien, Dmitry Kovalev, Gauthier Gidel.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stochastic Methods for Nonsmooth Optimization (Tuesday). Speakers: Tianyi Lin, Adil Salim and me.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Variational Inequalities, Minimax Problems and GANs (Part 2, Thursday). Speakers: Daoli Zhu,  Sarath Pattathil,  Georgios Piliouras.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More details about the conference can be found here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://iccopt2019.berlin/&#34; target=&#34;_blank&#34;&gt;https://iccopt2019.berlin/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The speakers and the titles of their presentations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Simon Lacoste-Julien &amp;ldquo;Negative Momentum for Improved Game Dynamics&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dmitry Kovalev &amp;ldquo;Revisiting Stochastic Extragradient Method&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gauthier Gidel &amp;ldquo;New Optimization Perspectives on Generative Adversarial Networks&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Daoli Zhu &amp;ldquo;A Variational Approach on Level sets and Linear Convergence of Variable Bregman Proximal Gradient Method for Nonconvex Optimization Problems&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sarath Pattathil &amp;ldquo;A unified analysis of optimistic gradient and extra-gradient methods for saddle point problems&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Georgios Piliouras &amp;ldquo;Online Optimization in Zero - Sum Games and Beyond: A Dynamical Systems Approach&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tianyi Lin &amp;ldquo;On Gradient Descent Ascent for Nonconvex-Concave Minimax Optimization&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adil Salim &amp;ldquo;Stochastic Proximal Langevin Algorithm&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Konstantin Mishchenko &amp;ldquo;Variance Reduction for Sums with Smooth and Nonsmooth Components with Linear Convergence&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;P.S. Staying in Berlin was as fanstastic as the conference, it is a lovely city and German beer is very helpful for fruitful discussions! :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I am at Simons Institute until 18 July</title>
      <link>https://konstmish.github.io/post/2019_simons/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/post/2019_simons/</guid>
      <description>&lt;p&gt;I am an attendee of the Frontiers of Deep Learning workshop at Simons Insitute for the Theory of Computing. There many well-known researchers here and just yesterday Yuanzhi Li gave a fantastic talk titled &amp;ldquo;Learning and Generalization in Over-parametrized Neural Networks, Going Beyond Kernels&amp;rdquo;. He presented a very simple explanation of what I believed to be very complicated and I encourage everyone interested in the topic to watch his presentation (should appear on youtube shortly).&lt;/p&gt;

&lt;p&gt;The workshop website is here: &lt;a href=&#34;https://simons.berkeley.edu/programs/dl2019&#34; target=&#34;_blank&#34;&gt;https://simons.berkeley.edu/programs/dl2019&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I visited Matthias Ehrhardt at Bath University</title>
      <link>https://konstmish.github.io/post/2019_bath/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/post/2019_bath/</guid>
      <description>&lt;p&gt;Matthias Ehrhardt is a prize fellow at Bath university and I first met him when I visited Cambridge in 2017-2018. Back then I was just a first-year PhD student and it was hard for me to fully understand his work on stochastic primal-dual hybrid gradient (&lt;a href=&#34;https://arxiv.org/abs/1706.04957&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1706.04957&lt;/a&gt;). However, I have more experience now and just in two weeks I managed to do a nice extension of my recent work (&lt;a href=&#34;https://arxiv.org/abs/1905.11535&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1905.11535&lt;/a&gt;) to the same setting as in the work by Matthias. The work is motivated by imaging application and I hope it will find important applications in practice.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I was at ICML 2019 presenting the work I did at Amazon</title>
      <link>https://konstmish.github.io/post/icml-2019_ts/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/post/icml-2019_ts/</guid>
      <description>&lt;p&gt;Our work on time series was accepted as a poster to the Time Series Workshop at ICML and I presented it together with Federico Vaggi. The conference was in Los Angeles from 9 to 15 June. I attended it with many other members of our group: Aritra Dutta, Samuel Horvath, Nicolas Loizou, Alibek Sailanbayev, Adil Selim and of course Peter Richt√°rik.&lt;/p&gt;

&lt;p&gt;You can find the website of the workshop and my work here: &lt;a href=&#34;http://roseyu.com/time-series-workshop/&#34; target=&#34;_blank&#34;&gt;http://roseyu.com/time-series-workshop/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Self-supervised Approach to Hierarchical Forecasting with Applications to Groupwise Synthetic Controls</title>
      <link>https://konstmish.github.io/publication/time_series/</link>
      <pubDate>Tue, 25 Jun 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/publication/time_series/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MISO is Making a Comeback With Better Proofs and Rates</title>
      <link>https://konstmish.github.io/publication/miso/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/publication/miso/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate</title>
      <link>https://konstmish.github.io/publication/dave_qn/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://konstmish.github.io/publication/dave_qn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Stochastic Decoupling Method for Minimizing the Sum of Smooth and Non-Smooth Functions</title>
      <link>https://konstmish.github.io/publication/stoch_decoupling/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/publication/stoch_decoupling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Revisiting Stochastic Extragradient</title>
      <link>https://konstmish.github.io/publication/extra_vi/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/publication/extra_vi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stochastic Distributed Learning with Gradient Quantization and Variance Reduction</title>
      <link>https://konstmish.github.io/publication/quant_vr/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/publication/quant_vr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I am a member of the program committee for NeurIPS and UAI 2019</title>
      <link>https://konstmish.github.io/post/19_neurips_uai/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 -0700</pubDate>
      
      <guid>https://konstmish.github.io/post/19_neurips_uai/</guid>
      <description>&lt;p&gt;After a successful round of reviews for International Conference on Machine Learning (ICML) I was invited to serve on committee for two more important ML conferences: Conference on Neural Information Processing Systems (NeurIPS) and Uncertainty in Artificial Intelligence (UAI). The review period for UAI is from 21 March to 21 April 2019 and all paper submissions have already been done, while for NeurIPS the deadline to submit a paper is on 23 May 2019 and the review period is from 18 June to 15 July 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I&#39;m visiting Martin Jaggi from 18 February to 15 March</title>
      <link>https://konstmish.github.io/post/18_epfl/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 -0800</pubDate>
      
      <guid>https://konstmish.github.io/post/18_epfl/</guid>
      <description>&lt;p&gt;There is no specific project to work on yet, but my research interests are almost a subset of what Martin and his group have worked on. I would not be surprised if it leads to more than one project and hope that this visit will not be the last one :).&lt;/p&gt;

&lt;p&gt;As of today, the group&amp;rsquo;s website is available at &lt;a href=&#34;https://mlo.epfl.ch/&#34; target=&#34;_blank&#34;&gt;https://mlo.epfl.ch/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our group photo</title>
      <link>https://konstmish.github.io/post/18_group/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 -0800</pubDate>
      
      <guid>https://konstmish.github.io/post/18_group/</guid>
      <description>


&lt;figure&gt;

&lt;img src=&#34;featured.jpg&#34; /&gt;


&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it</title>
      <link>https://konstmish.github.io/publication/99/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 -0800</pubDate>
      
      <guid>https://konstmish.github.io/publication/99/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distributed Learning with Compressed Gradient Differences</title>
      <link>https://konstmish.github.io/publication/diana/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 -0800</pubDate>
      
      <guid>https://konstmish.github.io/publication/diana/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
