<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Konstantin Mishchenko on Konstantin Mishchenko</title>
    <link>https://konstmish.github.io/</link>
    <description>Recent content in Konstantin Mishchenko on Konstantin Mishchenko</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Konstantin Mishchenko</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0300</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>MISO is Making a Comeback With Better Proofs and Rates</title>
      <link>https://konstmish.github.io/publication/miso/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/miso/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate</title>
      <link>https://konstmish.github.io/publication/dave_qn/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://konstmish.github.io/publication/dave_qn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Stochastic Decoupling Method for Minimizing the Sum of Smooth and Non-Smooth Functions</title>
      <link>https://konstmish.github.io/publication/stoch_decoupling/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/stoch_decoupling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Revisiting Stochastic Extragradient</title>
      <link>https://konstmish.github.io/publication/extra_vi/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/extra_vi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stochastic Distributed Learning with Gradient Quantization and Variance Reduction</title>
      <link>https://konstmish.github.io/publication/quant_vr/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/quant_vr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I am a member of the program committee for NeurIPS and UAI 2019</title>
      <link>https://konstmish.github.io/post/19_neurips_uai/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/post/19_neurips_uai/</guid>
      <description>&lt;p&gt;After a successful round of reviews for International Conference on Machine Learning (ICML) I was invited to serve on committee for two more important ML conferences: Conference on Neural Information Processing Systems (NeurIPS) and Uncertainty in Artificial Intelligence (UAI). The review period for UAI is from 21 March to 21 April 2019 and all paper submissions have already been done, while for NeurIPS the deadline to submit a paper is on 23 May 2019 and the review period is from 18 June to 15 July 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I&#39;m visiting Martin Jaggi from 18 February to 15 March</title>
      <link>https://konstmish.github.io/post/18_epfl/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/post/18_epfl/</guid>
      <description>&lt;p&gt;There is no specific project to work on yet, but my research interests are almost a subset of what Martin and his group have worked on. I would not be surprised if it leads to more than one project and hope that this visit will not be the last one :).&lt;/p&gt;

&lt;p&gt;As of today, the group&amp;rsquo;s website is available at &lt;a href=&#34;https://mlo.epfl.ch/&#34; target=&#34;_blank&#34;&gt;https://mlo.epfl.ch/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our group photo</title>
      <link>https://konstmish.github.io/post/18_group/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/post/18_group/</guid>
      <description>


&lt;figure&gt;

&lt;img src=&#34;featured.jpg&#34; /&gt;


&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it</title>
      <link>https://konstmish.github.io/publication/99/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/99/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distributed Learning with Compressed Gradient Differences</title>
      <link>https://konstmish.github.io/publication/diana/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/diana/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Stochastic Penalty Model for Convex and Nonconvex Optimization with Big Constraints</title>
      <link>https://konstmish.github.io/publication/penalty/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/publication/penalty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>I am a program committee member for ICML 2019!</title>
      <link>https://konstmish.github.io/post/icml-2019/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/post/icml-2019/</guid>
      <description>&lt;p&gt;This is my first time being a reviewer for such a venue, so I hope I will not be &amp;ldquo;that stupid reviewer&amp;rdquo; :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prize winner at IEEEXtreme</title>
      <link>https://konstmish.github.io/post/ieeextreme-2018/</link>
      <pubDate>Sun, 21 Oct 2018 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/post/ieeextreme-2018/</guid>
      <description>&lt;p&gt;In a 24 hours programming competition Alibek and I were solving tough mathematical and programming problems for the sake of fun and, &lt;em&gt;maybe&lt;/em&gt;, some prizes. Fortunately, we managed to make it to top 100, which means that we are eligible for prizes from IEEE!&lt;/p&gt;

&lt;p&gt;Last year I participated in the same competition with Samuel Horvath, but back then we were only 123rd.&lt;/p&gt;

&lt;p&gt;Link to the results: &lt;a href=&#34;https://ieeextreme.org/wp-content/uploads/2018/11/IEEEXtreme-12.0-Global-Ranking.pdf&#34; target=&#34;_blank&#34;&gt;https://ieeextreme.org/wp-content/uploads/2018/11/IEEEXtreme-12.0-Global-Ranking.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SEGA: Variance Reduction via Gradient Sketching</title>
      <link>https://konstmish.github.io/publication/sega/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://konstmish.github.io/publication/sega/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A NIPS paper got accepted!</title>
      <link>https://konstmish.github.io/post/nips-2018/</link>
      <pubDate>Thu, 30 Aug 2018 00:00:00 +0300</pubDate>
      
      <guid>https://konstmish.github.io/post/nips-2018/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m happy to say that our paper &amp;ldquo;SEGA: Variance Reduction via Gradient Sketching&amp;rdquo; about variance reduction for coordinate descent methods was accepted to NIPS. In this work, we discover how coordinate descent methods can be extended to problems with non-separable regularizer. We consider minibatch, importance sampling and acceleration of the produced method. What&amp;rsquo;s more, we additionally introduced a variant with non-trivial sketches of the gradient, under maximal possible stepsizes can be larger by an arbitrary number than in classical coordinate descent methods.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
