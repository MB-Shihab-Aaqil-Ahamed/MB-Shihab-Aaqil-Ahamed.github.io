[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD candidate supervised by Peter Richtárik, working on Optimization and its applications to Machine Learning. Prior to this, I was a research intern at Université Grenoble Alpes, where I worked with F. Iutzeler and J. Malick. This internship allowed me to obtain my double degree MSc diploma from École Normale Supérieure Paris-Saclay and Paris-Dauphine. I obtained my BSc from Moscow Institute of Physics and Technology, where I was having a lot of fun with quantum mechanics and various branches of classical and modern physics.\nMy hobbies include squash, bouldering and competitive programming.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1592476916,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://konstmish.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a PhD candidate supervised by Peter Richtárik, working on Optimization and its applications to Machine Learning. Prior to this, I was a research intern at Université Grenoble Alpes, where I worked with F. Iutzeler and J. Malick. This internship allowed me to obtain my double degree MSc diploma from École Normale Supérieure Paris-Saclay and Paris-Dauphine. I obtained my BSc from Moscow Institute of Physics and Technology, where I was having a lot of fun with quantum mechanics and various branches of classical and modern physics.","tags":null,"title":"Konstantin Mishchenko","type":"authors"},{"authors":null,"categories":null,"content":"Over the past two years, I have reviewed for NeurIPS, ICML, AAAI, UAI, and several journals. Due to the crazy number of submissions each of these conferences receives, all people with expertise should consider devoting part of their time to reviewing. This year, I\u0026rsquo;m also reviewing for ICLR, even though I haven\u0026rsquo;t submitted a single paper there. But since last year they received 2594 submissions, and taking into account the dynamics, it is very likely this year the number of submissions will be close to 4000. In that case, it seems important even people who have not previously submitted to ICLR review, given they have time.\n","date":1594414800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594478074,"objectID":"6d0c0015de080a5779ee3b4dc1434c83","permalink":"https://konstmish.github.io/post/20_iclr_review/","publishdate":"2020-07-11T00:00:00+03:00","relpermalink":"/post/20_iclr_review/","section":"post","summary":"Over the past two years, I have reviewed for NeurIPS, ICML, AAAI, UAI, and several journals. Due to the crazy number of submissions each of these conferences receives, all people with expertise should consider devoting part of their time to reviewing. This year, I'm also reviewing for ICLR, even though I haven't submitted a single paper there. But since last year they received 2594 submissions, and taking into account the dynamics, it is very likely this year the number of submissions will be close to 4000. In that case, it seems important even people who have not previously submitted to ICLR review, given they have time.\n","tags":[],"title":"I'll be a reviewer for ICLR 2021","type":"post"},{"authors":null,"categories":null,"content":"Peter Richtárik, Filip Hanzely and I are organizing a session on Optimization for Deep Learning at the SIAM Conference on Mathematics of Datascience (MDS20). See the link below for more info: https://www.siam.org/conferences/cm/conference/mds20\nOur presenters are:\nSimon Shaolei Du from the Institute for Advanced Study of Princeton (http://simonshaoleidu.com/)\nHaihao Lu from Google Research NYC (http://web.mit.edu/haihao/www/)\nSai Praneeth Karimireddy from EPFL (https://people.epfl.ch/sai.karimireddy)\nFilip Hanzely from KAUST (https://fhanzely.github.io/)\n","date":1593118800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594478074,"objectID":"bb1f5ae6bf6d48100634e10dfa308941","permalink":"https://konstmish.github.io/post/20_mds_conf/","publishdate":"2020-06-26T00:00:00+03:00","relpermalink":"/post/20_mds_conf/","section":"post","summary":"Peter Richtárik, Filip Hanzely and I are organizing a session on Optimization for Deep Learning at the SIAM Conference on Mathematics of Datascience (MDS20). See the link below for more info:\nhttps://www.siam.org/conferences/cm/conference/mds20\n\nOur presenters are:\n\nSimon Shaolei Du from the Institute for Advanced Study of Princeton (http://simonshaoleidu.com/)\n\nHaihao Lu from Google Research NYC (http://web.mit.edu/haihao/www/)\n\nSai Praneeth Karimireddy from EPFL (https://people.epfl.ch/sai.karimireddy)\n\nFilip Hanzely from KAUST (https://fhanzely.github.io/)\n","tags":[],"title":"Our session on Optimization for Deep Learning at MDS Conference","type":"post"},{"authors":["Konstantin Mishchenko","Ahmed Khaled","Peter Richtárik"],"categories":[],"content":"","date":1591822800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591998007,"objectID":"a79087189d1ef00603f64316fbe78880","permalink":"https://konstmish.github.io/publication/20_rr/","publishdate":"2020-06-11T00:00:00+03:00","relpermalink":"/publication/20_rr/","section":"publication","summary":"Random Reshuffling (RR) is an algorithm for minimizing finite-sum functions that utilizes iterative gradient descent steps in conjunction with data reshuffling. Often contrasted with its sibling Stochastic Gradient Descent (SGD), RR is usually faster in practice and enjoys significant popularity in convex and non-convex optimization. The convergence rate of RR has attracted substantial attention recently and, for strongly convex and smooth functions, it was shown to converge faster than SGD if 1) the stepsize is small, 2) the gradients are bounded, and 3) the number of epochs is large. We remove these 3 assumptions, improve the dependence on the condition number from $\\kappa^2$ to $\\kappa$ (resp. from $\\kappa$ to $\\sqrt{\\kappa}$) and, in addition, show that RR has a different type of variance. We argue through theory and experiments that the new variance type gives an additional justification of the superior performance of RR. To go beyond strong convexity, we present several results for non-strongly convex and non-convex objectives. We show that in all cases, our theory improves upon existing literature. Finally, we prove fast convergence of the Shuffle-Once (SO) algorithm, which shuffles the data only once, at the beginning of the optimization process. Our theory for strongly-convex objectives tightly matches the known lower bounds for both RR and SO and substantiates the common practical heuristic of shuffling once or only a few times. As a byproduct of our analysis, we also get new results for the Incremental Gradient algorithm (IG), which does not shuffle the data at all.","tags":[],"title":"Random Reshuffling: Simple Analysis with Vast Improvements","type":"publication"},{"authors":null,"categories":null,"content":"Our work with Yura Malitsky \u0026ldquo;Adaptive Gradient Descent without Descent\u0026rdquo; got accepted ICML with scores \u0026lsquo;Accept\u0026rsquo;, \u0026lsquo;Accept\u0026rsquo; and \u0026lsquo;Weak Accept\u0026rsquo;. You can see the arxiv version of the paper here: https://arxiv.org/abs/1910.09529. Also wait for our follow-up work that we hope to release in the next few months!\n","date":1590958800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591954344,"objectID":"66dec0f865d9356cb6e5a87e71555f28","permalink":"https://konstmish.github.io/post/20_icml_accept/","publishdate":"2020-06-01T00:00:00+03:00","relpermalink":"/post/20_icml_accept/","section":"post","summary":"Our work with Yura Malitsky \"Adaptive Gradient Descent without Descent\" got accepted ICML with scores 'Accept', 'Accept' and 'Weak Accept'. You can see the arxiv version of the paper here: https://arxiv.org/abs/1910.09529. Also wait for our follow-up work that we hope to release in the next few months!\n","tags":[],"title":"Adaptive Gradient got accepted to ICML","type":"post"},{"authors":null,"categories":null,"content":"This year we (Filip, Peter and I) submitted one paper to the conference on Uncertainty in Artificial Intelligence (UAI). The paper got overwhelmiingly positive feedback and will be presented virtually this summer. The current title of the paper is \u0026ldquo;99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it\u0026rdquo;, but following the reviewers\u0026rsquo; suggestions, the title will be changed to avoid confusion about its content.\n","date":1589922000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589986873,"objectID":"17c53c34b233c22d20108ec4e350ab05","permalink":"https://konstmish.github.io/post/20_uai_accept/","publishdate":"2020-05-20T00:00:00+03:00","relpermalink":"/post/20_uai_accept/","section":"post","summary":"This year we (Filip, Peter and I) submitted one paper to the conference on Uncertainty in Artificial Intelligence (UAI). The paper got overwhelmiingly positive feedback and will be presented virtually this summer. The current title of the paper is \"99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it\", but following the reviewers' suggestions, the title will be changed to avoid confusion about its content.\n","tags":[],"title":"UAI: paper accepted (1 strong accept and 4 accepts)","type":"post"},{"authors":["Adil Salim","Laurent Condat","Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1585861200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586269806,"objectID":"2f4987d0aedcb473a14af8f6df512ed3","permalink":"https://konstmish.github.io/publication/20_pddy/","publishdate":"2020-04-03T00:00:00+03:00","relpermalink":"/publication/20_pddy/","section":"publication","summary":"We introduce a new primal-dual algorithm for minimizing the sum of three convex functions, each of which has its own oracle. Namely, the first one is differentiable, smooth and possibly stochastic, the second is proximable, and the last one is a composition of a proximable function with a linear map. Our theory covers several settings that are not tackled by any existing algorithm; we illustrate their importance with real-world applications. By leveraging variance reduction, we obtain convergence with linear rates under strong convexity and fast sublinear convergence under convexity assumptions. The proposed theory is simple and unified by the umbrella of stochastic Davis-Yin splitting, which we design in this work. Finally, we illustrate the efficiency of our method through numerical experiments. ","tags":[],"title":"Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms","type":"publication"},{"authors":null,"categories":null,"content":"At the time when there is nothing to do, no place to travel to and no party to have fun at, reviewing papers can be a good way to kill time. I just finished my reviews for ICML, and in 2 weeks there will be another deadline, reviews for UAI. I think I have never spent so much time reviewing as with this ICML, mostly because usually there are a lot of things going on around, but not now thanks to coronavirus.\n","date":1584738000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586208821,"objectID":"6690a56ab337c1be8f50ba2f0da07802","permalink":"https://konstmish.github.io/post/20_icml_reviews/","publishdate":"2020-03-21T00:00:00+03:00","relpermalink":"/post/20_icml_reviews/","section":"post","summary":"At the time when there is nothing to do, no place to travel to and no party to have fun at, reviewing papers can be a good way to kill time. I just finished my reviews for ICML, and in 2 weeks there will be another deadline, reviews for UAI. I think I have never spent so much time reviewing as with this ICML, mostly because usually there are a lot of things going on around, but not now thanks to coronavirus.\n","tags":[],"title":"ICML reviews done","type":"post"},{"authors":null,"categories":null,"content":"I was selected as one of 12 Outstanding Program Committee members of AAAI, out of more than 6000 reviewers in total. There were about 7737 submissions, which is 10% more than that of NeurIPS. The award will be officially given to me and the other outstanding reviewers on 11 February at 8am. I\u0026rsquo;m looking forward to going to New York for the conference!\n","date":1579640400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582324189,"objectID":"4ba0f6f042f899bdab308de231e7ce03","permalink":"https://konstmish.github.io/post/20_aaai/","publishdate":"2020-01-22T00:00:00+03:00","relpermalink":"/post/20_aaai/","section":"post","summary":"I was selected as one of 12 Outstanding Program Committee members of AAAI, out of more than 6000 reviewers in total. There were about 7737 submissions, which is 10% more than that of NeurIPS. The award will be officially given to me and the other outstanding reviewers on 11 February at 8am. I'm looking forward to going to New York for the conference!\n","tags":[],"title":"I'm one of 12 Outstanding Reviewers for AAAI","type":"post"},{"authors":null,"categories":null,"content":"Today we received the final decisions for our papers submitted to the AISTATS conference (https://www.aistats.org/). Although one work was rejected, this constitutes a good acceptance rate. Unfortunately, we also had to withdraw one of our submissions. Below is the list of papers that we will present:\n Revisiting Stochastic Extragradient (K. Mishchenko, D. Kovalev, E. Shulgin, Y. Malitsky and P. Richtárik)\n Tighter Theory for Local SGD on Identical and Heterogeneous Data (A. Khaled, K. Mishchenko and P. Richtárik)\n DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate (S. Soori, K. Mishchenko, A. Mokhtari, M. Dehnavi and M. Gürbüzbalaban)\n  Please drop by our posters in Palermo, Italy on 3-5 June!\n","date":1578344400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586208821,"objectID":"6b017ead7998ac67e4a809d1a13540c2","permalink":"https://konstmish.github.io/post/20_aistats/","publishdate":"2020-01-07T00:00:00+03:00","relpermalink":"/post/20_aistats/","section":"post","summary":"Today we received the final decisions for our papers submitted to the AISTATS conference (https://www.aistats.org/). Although one work was rejected, this constitutes a good acceptance rate. Unfortunately, we also had to withdraw one of our submissions. Below is the list of papers that we will present:\n\n1. Revisiting Stochastic Extragradient (K. Mishchenko, D. Kovalev, E. Shulgin, Y. Malitsky and P. Richtárik)\n\n2. Tighter Theory for Local SGD on Identical and Heterogeneous Data (A. Khaled, K. Mishchenko, P. Richtárik)\n\n3. DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate (S. Soori, K. Mishchenko, A. Mokhtari, M. Dehnavi, M. Gürbüzbalaban)\n\nPlease drop by our posters in Palermo, Italy on 3-5 June!\n","tags":[],"title":"AISTATS 2020: 3 papers accepted, 1 rejected","type":"post"},{"authors":null,"categories":null,"content":"One of the first papers that I wrote got accepted to the SIAM journal on optimization (SIOPT). The review process was quite long and included several revisions, but I\u0026rsquo;m happy I got it accepted before my graduation. This work is a result of my collaboration with J. Malick and F. Iutzeler, from whose experience I learned a lot about optimization. The title of the work is \u0026ldquo;A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm\u0026rdquo;.\n","date":1576098000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576195507,"objectID":"c015cac1e26fb0970bbde9b94fcc69e8","permalink":"https://konstmish.github.io/post/19_siam/","publishdate":"2019-12-12T00:00:00+03:00","relpermalink":"/post/19_siam/","section":"post","summary":"One of the first papers that I wrote got accepted to the SIAM journal on optimization (SIOPT). The review process was quite long and included several revisions, but I'm happy I got it accepted before my graduation. This work is a result of my collaboration with J. Malick and F. Iutzeler, from whose experience I learned a lot about optimization. The title of the work is \"A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm\".\n","tags":[],"title":"Paper got accepted to the SIAM Optimization journal","type":"post"},{"authors":null,"categories":null,"content":"In addition to my free NeurIPS registration, which I received as on of the top reviewers, I will also receive $1400 from the NeurIPS Foundation to sponsor my travel to the conference.\n","date":1575406800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576195608,"objectID":"494e46250fb47a85bb8d6cbbfd35b106","permalink":"https://konstmish.github.io/post/19_neurips_award_travel/","publishdate":"2019-12-04T00:00:00+03:00","relpermalink":"/post/19_neurips_award_travel/","section":"post","summary":"In addition to my free NeurIPS registration, which I received as on of the top reviewers, I will also receive $1400 from the NeurIPS Foundation to sponsor my travel to the conference.\n","tags":[],"title":"I got NeurIPS Travel Award","type":"post"},{"authors":["Dmitry Kovalev","Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1575320400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578486899,"objectID":"5b896b681e7b33db0c318828161b4780","permalink":"https://konstmish.github.io/publication/19_newton/","publishdate":"2019-12-03T00:00:00+03:00","relpermalink":"/publication/19_newton/","section":"publication","summary":"We present two new remarkably simple stochastic second-order methods for minimizing the average of a very large number of sufficiently smooth and strongly convex functions. The first is a stochastic variant of Newton's method (SN), and the second is a stochastic variant of cubically regularized Newton's method (SCN). We establish local linear-quadratic convergence results. Unlike existing stochastic variants of second order methods, which require the evaluation of a large number of gradients and/or Hessians in each iteration to guarantee convergence, our methods do not have this shortcoming. For instance, the simplest variants of our methods in each iteration need to compute the gradient and Hessian of a single randomly selected function only. In contrast to most existing stochastic Newton and quasi-Newton methods, our approach guarantees local convergence faster than with first-order oracle and adapts to the problem's curvature. Interestingly, our method is not unbiased, so our theory provides new intuition for designing new stochastic methods.","tags":[],"title":"Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates","type":"publication"},{"authors":null,"categories":null,"content":"I was invited by Boris Polyak to present my work on Sinkhorn algorithm at his seminar. The talk took place on Tuesday, 22 October, at the Institute of Control Sciences. It was a great pleasure to hear that Boris liked my work for its simplicity. The slides of my talk are now attached to the corresponding publication (https://konstmish.github.io/publication/19_sinkhorn/).\n","date":1572123600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572283341,"objectID":"9019e0aed46ebcb11cb50450ea9b46b0","permalink":"https://konstmish.github.io/post/19_polyak/","publishdate":"2019-10-27T00:00:00+03:00","relpermalink":"/post/19_polyak/","section":"post","summary":"I was invited by Boris Polyak to present my work on Sinkhorn algorithm at his seminar. The talk took place on Tuesday, 22 October, at the Institute of Control Sciences. It was a great pleasure to hear that Boris liked my work for its simplicity. The slides of my talk are now attached to the corresponding publication (https://konstmish.github.io/publication/19_sinkhorn/).\n","tags":[],"title":"Invited talk at Boris Polyak's seminar","type":"post"},{"authors":["Yura Malitsky","Konstantin Mishchenko"],"categories":[],"content":"","date":1571605200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597756299,"objectID":"dcf16583309abb4430989eee870afc31","permalink":"https://konstmish.github.io/publication/19_adgd/","publishdate":"2019-10-21T00:00:00+03:00","relpermalink":"/publication/19_adgd/","section":"publication","summary":"We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on smoothness in a neighborhood of a solution. Given that the problem is convex, our method will converge even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including matrix factorization and training of ResNet-18.","tags":[],"title":"Adaptive gradient descent without descent","type":"publication"},{"authors":null,"categories":null,"content":"We have submitted 5 papers to 4 different workshops hosted by NeurIPS and all of them were accepted, including one work for oral presentation. The list of papers:\n Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates (oral, with D. Kovalev and P. Richtárik)\n Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent (single-author work)\n Better Communication Complexity for Local SGD (with A. Khaled and P. Richtárik)\n First Analysis of Local GD on Heterogeneous Data (with A. Khaled and P. Richtárik)\n Revisiting Stochastic Extragradient (with D. Kovalev and P. Richtárik)\n  ","date":1569963600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578411414,"objectID":"0ed20735dd2a4579f5bb9f230bb3d940","permalink":"https://konstmish.github.io/post/19_workshops/","publishdate":"2019-10-02T00:00:00+03:00","relpermalink":"/post/19_workshops/","section":"post","summary":"We have submitted 5 papers to 4 different workshops hosted by NeurIPS and all of them were accepted, including one work for oral presentation. The list of papers:\n\n1. Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates (oral, with D. Kovalev and P. Richtárik)\n\n2. Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent (single-author work)\n\n3. Better Communication Complexity for Local SGD (with A. Khaled and P. Richtárik)\n\n4. First Analysis of Local GD on Heterogeneous Data (with A. Khaled and P. Richtárik)\n\n5. Revisiting Stochastic Extragradient (with D. Kovalev, E. Shulgin, Y. Malitsky and P. Richtárik)\n","tags":[],"title":"NeurIPS workshops: 5 accepted papers","type":"post"},{"authors":null,"categories":null,"content":"We just got a notification that our paper (by D. Kovalev, P. Richtárik and me) was accepted to the NeurIPS workshop \u0026ldquo;Beyond First-Order Optimization Methods in Machine Learning\u0026rdquo; for a spotlight (8 minute talk) and poster presentation. Together with the free registration that I got as one of the top reviewers, this gives more than enough reason to attend the conference. We are also waiting for the decisions for our submissions to other workshops, so soon there might be more news!\n","date":1569618000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569682811,"objectID":"fe265b54a419a6f505e601b3f8d2359c","permalink":"https://konstmish.github.io/post/19_neurips_newton/","publishdate":"2019-09-28T00:00:00+03:00","relpermalink":"/post/19_neurips_newton/","section":"post","summary":"We just got a notification that our paper (by D. Kovalev, P. Richtárik and me) was accepted to the NeurIPS workshop \"Beyond First-Order Optimization Methods in Machine Learning\" for a spotlight (8 minute talk) and poster presentation. Together with the free registration that I got as one of the top reviewers, this gives more than enough reason to attend the conference. We are also waiting for the decisions for our submissions to other workshops, so soon there might be more news!\n","tags":[],"title":"Paper accepted as spotlight to NeurIPS workshop 'Beyond First-Order Methods in ML'","type":"post"},{"authors":null,"categories":null,"content":"My new paper (for the first time I wrote a single authored work!) is now available online, see the publication section. It turned out the famous Sinkhorn algorithm is nothing but an instance of stochastic mirror descent. Very exciting to see the notion of relative smoothness appear as the only explanation of convergence from the mirror descent perspective.\n","date":1568667600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578411414,"objectID":"0fbd4ee5041d9e662d03357fd5ea307a","permalink":"https://konstmish.github.io/post/19_sinkhorn/","publishdate":"2019-09-17T00:00:00+03:00","relpermalink":"/post/19_sinkhorn/","section":"post","summary":"My new paper (for the first time I wrote a single authored work!) is now available online, see the publication section. It turned out the famous Sinkhorn algorithm is nothing but an instance of stochastic mirror descent. Very exciting to see the notion of relative smoothness appear as the only explanation of convergence from the mirror descent perspective.\n","tags":[],"title":"A new paper which connects Sinkhorn Algorithm and Stochastc Mirror Descent","type":"post"},{"authors":null,"categories":null,"content":"As I\u0026rsquo;ve done some research in the field of minmax optimization and deep learning, I was invited to be a reviewer for this year instance of the Smooth Games Optimization and Machine Learning Series of workshops.\n","date":1568667600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568738244,"objectID":"97e9edd905cdb2f43e01e4e6d624c244","permalink":"https://konstmish.github.io/post/19_neurips_games_reviewer/","publishdate":"2019-09-17T00:00:00+03:00","relpermalink":"/post/19_neurips_games_reviewer/","section":"post","summary":"As I've done some research in the field of minmax optimization and deep learning, I was invited to be a reviewer for this year instance of the Smooth Games Optimization and Machine Learning Series of workshops.\n","tags":[],"title":"Reviewer for the NeurIPS workshop 'Bridging Game Theory and Deep Learning'","type":"post"},{"authors":["Konstantin Mishchenko"],"categories":[],"content":"","date":1568581200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578486899,"objectID":"9b4d5e1fbb71f5a7f9aae54a7afc50ba","permalink":"https://konstmish.github.io/publication/19_sinkhorn/","publishdate":"2019-09-16T00:00:00+03:00","relpermalink":"/publication/19_sinkhorn/","section":"publication","summary":"We present a new perspective on the celebrated Sinkhorn algorithm by showing that is a special case of incremental/stochastic mirror descent. In order to see this, one should simply plug Kullback-Leibler divergence in both mirror map and the objective function. Since the problem has unbounded domain, the objective function is neither smooth nor it has bounded gradients. However, one can still approach the problem using the notion of relative smoothness, obtaining that the stochastic objective is 1-relative smooth. The discovered equivalence allows us to propose 1) new methods for optimal transport, 2) an extension of Sinkhorn algorithm beyond two constraints.","tags":[],"title":"Sinkhorn Algorithm as a Special Case of Stochastic Mirror Descent","type":"publication"},{"authors":null,"categories":null,"content":"This summer I collaborated with an intern at our group, Ahmed Khaled (these are just 2 of his 4 first names and he actually doesn\u0026rsquo;t have a last name as any othey Egyptian). The topic of these works is federated learning, which is de facto the standard way of training large models with data from mobile users. Despite its numerical success in certain applications, there are significant difficulties in applying it to setting with heterogeneous data, and in most cases data are not homogeneous. To address why this happens, we tried to tighten the existing theory and found out that our theoretical discoveries have a tight match with numerical experiments. We are still working on this, so more papers are to come!\n","date":1568322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568388362,"objectID":"3f027d782177bbff7b1aaef78abc7c59","permalink":"https://konstmish.github.io/post/19_fed_learning/","publishdate":"2019-09-13T00:00:00+03:00","relpermalink":"/post/19_fed_learning/","section":"post","summary":"We just uploaded two papers on federated learning to arxiv. The links are above on my website (\"Recent publications\").\n","tags":[],"title":"Two new papers on federated learning (and more are coming!)","type":"post"},{"authors":["Ahmed Khaled","Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1568235600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572689227,"objectID":"c536663a46dbcf3083d3ba0362f32f49","permalink":"https://konstmish.github.io/publication/19_local_gd/","publishdate":"2019-09-12T00:00:00+03:00","relpermalink":"/publication/19_local_gd/","section":"publication","summary":"We provide the first convergence analysis of local gradient descent for minimizing the average of smooth and convex but otherwise arbitrary functions. Problems of this form and local gradient descent as a solution method are of importance in federated learning, where each function is based on private data stored by a user on a mobile device, and the data of different users can be arbitrarily heterogeneous. We show that in a low accuracy regime, the method has the same communication complexity as gradient descent.","tags":[],"title":"First Analysis of Local GD on Heterogeneous Data","type":"publication"},{"authors":["Ahmed Khaled","Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1568235600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597767088,"objectID":"5f6533a22927b91e655a5d27b29014c5","permalink":"https://konstmish.github.io/publication/19_local_sgd/","publishdate":"2019-09-12T00:00:00+03:00","relpermalink":"/publication/19_local_sgd/","section":"publication","summary":"We revisit the local Stochastic Gradient Descent (local SGD) method and prove new convergence rates. We close the gap in the theory by showing that it works under unbounded gradients and extend its convergence to weakly convex functions. Furthermore, by changing the assumptions, we manage to get new bounds that explain in what regimes local SGD is faster that its non-local version. For instance, if the objective is strongly convex, we show that, up to constants, it is sufficient to synchronize M times in total, where M is the number of nodes. This improves upon the known requirement of Stich (2018) of sqrt(TM) synchronization times in total, where T is the total number of iterations, which helps to explain the empirical success of local SGD.","tags":[],"title":"Tighter Theory for Local SGD on Identical and Heterogeneous Data","type":"publication"},{"authors":null,"categories":null,"content":"I received free NeurIPS registration for providing high quality reviews. It is awarded to the top 400 reviewers, and some people call it \u0026ldquo;Best Reviewer Award\u0026rdquo;.\n","date":1567630800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567661208,"objectID":"3cf5ccc211ad436eb7d7f5b650b54df1","permalink":"https://konstmish.github.io/post/19_neurips_award/","publishdate":"2019-09-05T00:00:00+03:00","relpermalink":"/post/19_neurips_award/","section":"post","summary":"I received free NeurIPS registration for providing high quality reviews. It is awarded to the top 400 reviewers, and some people call it \"Best Reviewer Award\".\n","tags":[],"title":"NeurIPS 2019 Best Reviewer Award","type":"post"},{"authors":null,"categories":null,"content":"This year I am also serving as a reviewer for the AAAI conference, which will take place in February in New-York. See the official website for more details https://aaai.org/Conferences/AAAI-20/#.\n","date":1565902800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579645924,"objectID":"08497d380f63b91945804e48202fe90c","permalink":"https://konstmish.github.io/post/19_aaai-2/","publishdate":"2019-08-16T00:00:00+03:00","relpermalink":"/post/19_aaai-2/","section":"post","summary":"This year I am also serving as a reviewer for the AAAI conference, which will take place in February in New-York. See the official website for more details https://aaai.org/Conferences/AAAI-20/#.\n","tags":[],"title":"AAAI-20: Program Committee member","type":"post"},{"authors":null,"categories":null,"content":"I was at the ICCOPT conference in Berlin from 5 to 8 August as the chair of 3 sessions: 2 on variational inequality/minimax/GANs and 1 on non-smooth optimization.\nICCOPT is my favorite conference so far. It was completely devoted to optimization and the density of interesting talks was overwhelming, very often I could not decide to which parallel session I should go. I was lucky to meet with many brilliant people, whose work inspired some of my papers. I find the diversity of the presented topics quite high, and it is surprising how many different ideas and applications are covered by mathematical optimization.\nOrganizing was quite fun as well. Next time I may try to organize a couple more sessions to see if it is even more fun at a larger scale.\nMy 3 sessions were:\n Variational Inequalities, Minimax Problems and GANs (Part 1, Monday). Speakers: Simon Lacoste-Julien, Dmitry Kovalev, Gauthier Gidel.\n Stochastic Methods for Nonsmooth Optimization (Tuesday). Speakers: Tianyi Lin, Adil Salim and me.\n Variational Inequalities, Minimax Problems and GANs (Part 2, Thursday). Speakers: Daoli Zhu, Sarath Pattathil, Georgios Piliouras.\n  More details about the conference can be found here:\nhttps://iccopt2019.berlin/\nThe speakers and the titles of their presentations:\n Simon Lacoste-Julien \u0026ldquo;Negative Momentum for Improved Game Dynamics\u0026rdquo;\n Dmitry Kovalev \u0026ldquo;Revisiting Stochastic Extragradient Method\u0026rdquo;\n Gauthier Gidel \u0026ldquo;New Optimization Perspectives on Generative Adversarial Networks\u0026rdquo;\n Daoli Zhu \u0026ldquo;A Variational Approach on Level sets and Linear Convergence of Variable Bregman Proximal Gradient Method for Nonconvex Optimization Problems\u0026rdquo;\n Sarath Pattathil \u0026ldquo;A unified analysis of optimistic gradient and extra-gradient methods for saddle point problems\u0026rdquo;\n Georgios Piliouras \u0026ldquo;Online Optimization in Zero - Sum Games and Beyond: A Dynamical Systems Approach\u0026rdquo;\n Tianyi Lin \u0026ldquo;On Gradient Descent Ascent for Nonconvex-Concave Minimax Optimization\u0026rdquo;\n Adil Salim \u0026ldquo;Stochastic Proximal Langevin Algorithm\u0026rdquo;\n Konstantin Mishchenko \u0026ldquo;Variance Reduction for Sums with Smooth and Nonsmooth Components with Linear Convergence\u0026rdquo;\n  P.S. Staying in Berlin was as fanstastic as the conference, it is a lovely city and German beer is very helpful for fruitful discussions! :)\n","date":1565038800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"b6888b0a3fe2728ce7dc2b12a90ffbc7","permalink":"https://konstmish.github.io/post/19_iccopt/","publishdate":"2019-08-06T00:00:00+03:00","relpermalink":"/post/19_iccopt/","section":"post","summary":"I was at the ICCOPT conference in Berlin from 5 to 8 August as the chair of 3 sessions: 2 on variational inequality/minimax/GANs and 1 on non-smooth optimization.\n","tags":[],"title":"My sessions at ICCCOPT","type":"post"},{"authors":null,"categories":null,"content":"I am an attendee of the Frontiers of Deep Learning workshop at Simons Insitute for the Theory of Computing. There many well-known researchers here and just yesterday Yuanzhi Li gave a fantastic talk titled \u0026ldquo;Learning and Generalization in Over-parametrized Neural Networks, Going Beyond Kernels\u0026rdquo;. He presented a very simple explanation of what I believed to be very complicated and I encourage everyone interested in the topic to watch his presentation (should appear on youtube shortly).\nThe workshop website is here: https://simons.berkeley.edu/programs/dl2019\nUpdate: the talk by Yuanzhi Li is available on Youtube: https://www.youtube.com/watch?v=NNPCk2gvTnI\n","date":1563224400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570020598,"objectID":"9ee84ca0eb8627d4f7c7123ff079c43e","permalink":"https://konstmish.github.io/post/19_simons/","publishdate":"2019-07-16T00:00:00+03:00","relpermalink":"/post/19_simons/","section":"post","summary":"From 15 to 18 July I'm attending the Frontiers of Deep Learning workshop at Simons Insitute.\n","tags":[],"title":"I am at Simons Institute from 15 to 18 July","type":"post"},{"authors":null,"categories":null,"content":"Matthias Ehrhardt is a prize fellow at Bath university and I first met him when I visited Cambridge in 2017-2018. Back then I was just a first-year PhD student and it was hard for me to fully understand his work on stochastic primal-dual hybrid gradient (https://arxiv.org/abs/1706.04957). However, I have more experience now and just in two weeks I managed to do a nice extension of my recent work (https://arxiv.org/abs/1905.11535) to the same setting as in the work by Matthias. The work is motivated by imaging application and I hope it will find important applications in practice.\n","date":1563224400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"f8ddc97e7b52ccaf33e6e19591b436be","permalink":"https://konstmish.github.io/post/19_bath/","publishdate":"2019-07-16T00:00:00+03:00","relpermalink":"/post/19_bath/","section":"post","summary":"From 17 to 28 June I visited Matthias Ehrhardt.\n","tags":[],"title":"I visited Matthias Ehrhardt at Bath University","type":"post"},{"authors":null,"categories":null,"content":"Our work on time series was accepted as a poster to the Time Series Workshop at ICML and I presented it together with Federico Vaggi. The conference was in Los Angeles from 9 to 15 June. I attended it with many other members of our group: Aritra Dutta, Samuel Horvath, Nicolas Loizou, Alibek Sailanbayev, Adil Selim and of course Peter Richtárik.\nYou can find the website of the workshop and my paper here: http://roseyu.com/time-series-workshop/\n","date":1563224400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"953bba99ccc3da1016bea122ad6309f6","permalink":"https://konstmish.github.io/post/19_icml_ts/","publishdate":"2019-07-16T00:00:00+03:00","relpermalink":"/post/19_icml_ts/","section":"post","summary":"Our work on time series was accepted as a poster to the Time Series Workshop at ICML and I presented it together with Federico Vaggi.\n","tags":[],"title":"I was at ICML 2019 presenting the work I did at Amazon","type":"post"},{"authors":["Konstantin Mishchenko","Mallory Montgomery","Federico Vaggi"],"categories":[],"content":"","date":1561410000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568744518,"objectID":"19399cf5b66e5dbde0261dda48ceb53c","permalink":"https://konstmish.github.io/publication/19_time_series/","publishdate":"2019-06-25T00:00:00+03:00","relpermalink":"/publication/19_time_series/","section":"publication","summary":"When forecasting time series with a hierarchical structure, the existing state of the art is to forecast each time series independently, and, in a post-treatment step, to reconcile the time series in a way that respects the hierarchy (Hyndman et al., 2011; Wickramasuriya et al., 2018). We propose a new loss function that can be incorporated into any maximum likelihood objective with hierarchical data, resulting in reconciled estimates with confidence intervals that correctly account for additional uncertainty due to imperfect reconciliation. We evaluate our method using a non-linear model and synthetic data on a counterfactual forecasting problem, where we have access to the ground truth and contemporaneous covariates, and show that we largely improve over the existing state-of-the-art method.","tags":[],"title":"A Self-supervised Approach to Hierarchical Forecasting with Applications to Groupwise Synthetic Controls","type":"publication"},{"authors":["Xun Qian","Alibek Sailanbayev","Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1559595600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568744518,"objectID":"9e2f07b3420009bceb107d414536f0ad","permalink":"https://konstmish.github.io/publication/19_miso/","publishdate":"2019-06-04T00:00:00+03:00","relpermalink":"/publication/19_miso/","section":"publication","summary":"MISO, also known as Finito, was one of the first stochastic variance reduced methods discovered, yet its popularity is fairly low. Its initial analysis was significantly limited by the so-called Big Data assumption. Although the assumption was lifted in subsequent work using negative momentum, this introduced a new parameter and required knowledge of strong convexity and smoothness constants, which is rarely possible in practice. We rehabilitate the method by introducing a new variant that needs only smoothness constant and does not have any extra parameters. Furthermore, when removing the strong convexity constant from the stepsize, we present a new analysis of the method, which no longer uses the assumption that every component is strongly convex. This allows us to also obtain so far unknown nonconvex convergence of MISO. To make the proposed method efficient in practice, we derive minibatching bounds with arbitrary uniform sampling that lead to linear speedup when the expected minibatch size is in a certain range. Our numerical experiments show that MISO is a serious competitor to SAGA and SVRG and sometimes outperforms them on real datasets.","tags":[],"title":"MISO is Making a Comeback With Better Proofs and Rates","type":"publication"},{"authors":["Saeed Soori","Konstantin Mishchenko","Aryan Mokhtari","Maryam Mehri Dehnavi","Mert Gürbüzbalaban"],"categories":null,"content":"","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597767088,"objectID":"34d3d7cc7b952fc8ba5f912d69b837fa","permalink":"https://konstmish.github.io/publication/19_dave_qn/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/publication/19_dave_qn/","section":"publication","summary":"In this paper, we consider distributed algorithms for solving the empirical risk minimization problem under the master/worker communication model. We develop a distributed asynchronous quasi-Newton algorithm that can achieve superlinear convergence. To our knowledge, this is the first distributed asynchronous algorithm with superlinear convergence guarantees. Our algorithm is communicationefficient in the sense that at every iteration the master node and workers communicate vectors of size O(p), where p is the dimension of the decision variable. The proposed method is based on a distributed asynchronous averaging scheme of decision vectors and gradients in a way to effectively capture the local Hessian information of the objective function. Our convergence theory supports asynchronous computations subject to both bounded delays and unbounded delays with a bounded time-average. Unlike in the majority of asynchronous optimization literature, we do not require choosing smaller stepsize when delays are huge. We provide numerical experiments that match our theoretical results and showcase significant improvement comparing to state-of-the-art distributed algorithms.","tags":null,"title":"DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate","type":"publication"},{"authors":["Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1558904400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572689227,"objectID":"9fe0b45fd9f638b06074f092e96a4135","permalink":"https://konstmish.github.io/publication/19_stoch_decoupling/","publishdate":"2019-05-27T00:00:00+03:00","relpermalink":"/publication/19_stoch_decoupling/","section":"publication","summary":"We consider the problem of minimizing the sum of three convex functions: i) a smooth function $f$ in the form of an expectation or a finite average, ii) a non-smooth function $g$ in the form of a finite average of proximable functions $g_j$, and iii) a proximable regularizer $R$. We design a variance reduced method which is able progressively  learn the proximal operator of $g$ via the computation of the proximal operator of a single randomly selected function $g_j$ in each iteration only. Our method can provably and efficiently accommodate many strategies for the estimation of the gradient of $f$, including via standard and variance-reduced stochastic estimation, effectively decoupling the smooth part of the problem from the non-smooth part. We prove a number of iteration complexity results, including a general $O(1/t)$ rate, $O(1/t^2)$ rate in the case of strongly convex $f$, and several linear rates in special cases, including accelerated linear rate. For example, our method achieves a linear rate for the problem of minimizing a strongly convex function $f$ under linear constraints under no assumption on the constraints beyond consistency. When combined with SGD or SAGA estimators for the gradient of $f$, this  leads to  a very efficient method for empirical risk minimization with large linear constraints.  Our method generalizes several existing algorithms, including forward-backward splitting, Douglas-Rachford splitting, proximal SGD, proximal SAGA, SDCA, randomized Kaczmarz and Point-SAGA. However, our method leads to many new specific methods in special cases; for instance,  we obtain the first randomized variant of the Dykstra's method for projection onto the intersection of closed convex sets. ","tags":[],"title":"A Stochastic Decoupling Method for Minimizing the Sum of Smooth and Non-Smooth Functions","type":"publication"},{"authors":["Konstantin Mishchenko","Dmitry Kovalev","Egor Shulgin","Peter Richtárik","Yura Malitsky"],"categories":[],"content":"","date":1558904400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597767088,"objectID":"8f5a4ae45dd5b85e44d77e8b9094a6a2","permalink":"https://konstmish.github.io/publication/19_extra_vi/","publishdate":"2019-05-27T00:00:00+03:00","relpermalink":"/publication/19_extra_vi/","section":"publication","summary":"We consider a new extension of the extragradient method that is motivated by approximating implicit updates. Since in a recent work [chavdarova2019reducing] it was shown that the existing stochastic extragradient algorithm (called mirror-prox) of [juditsky2011solving] diverges on a simple bilinear problem, we prove guarantees for solving variational inequality that are more general than in [juditsky2011solving]. Furthermore, we illustrate numerically that the proposed variant converges faster than many other methods on the example of [chavdarova2019reducing]. We also discuss how extragradient can be applied to training Generative Adversarial Networks (GANs). Our experiments on GANs demonstrate that the introduced approach may make the training faster in terms of data passes, while its higher iteration complexity makes the advantage smaller. To further accelerate method's convergence on problems such as bilinear minimax, we combine the extragradient step with negative momentum [gidel2018negative] and discuss the optimal momentum value.","tags":[],"title":"Revisiting Stochastic Extragradient","type":"publication"},{"authors":["Samuel Horváth","Dmitry Kovalev","Konstantin Mishchenko","Sebastian Stich","Peter Richtárik"],"categories":[],"content":"","date":1554930000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568744518,"objectID":"724953a1ea520f92ce1f31d49f365889","permalink":"https://konstmish.github.io/publication/19_quant_vr/","publishdate":"2019-04-11T00:00:00+03:00","relpermalink":"/publication/19_quant_vr/","section":"publication","summary":"We consider distributed optimization where the objective function is spread among different devices, each sending incremental model updates to a central server. To alleviate the communication bottleneck, recent work proposed various schemes to compress (e.g. quantize or sparsify) the gradients, thereby introducing additional variance $ \\omega\\ge $ 1 that might slow down convergence. For strongly convex functions with condition number kappa distributed among $n$ machines, we (i) give a scheme that converges in $O((\\kappa+\\kappa \\omega / n+\\omega) \\log(1/\\epsilon))$ steps to a neighborhood of the optimal solution. For objective functions with a finite-sum structure, each worker having less than m components, we (ii) present novel variance reduced schemes that converge in $O((\\kappa+\\kappa \\omega / n+\\omega+m)log(1/\\epsilon))$ steps to arbitrary accuracy $\\epsilon0$.","tags":[],"title":"Stochastic Distributed Learning with Gradient Quantization and Variance Reduction","type":"publication"},{"authors":null,"categories":null,"content":"After a successful round of reviews for International Conference on Machine Learning (ICML) I was invited to serve on committee for two more important ML conferences: Conference on Neural Information Processing Systems (NeurIPS) and Uncertainty in Artificial Intelligence (UAI). The review period for UAI is from 21 March to 21 April 2019 and all paper submissions have already been done, while for NeurIPS the deadline to submit a paper is on 23 May 2019 and the review period is from 18 June to 15 July 2019.\n","date":1552338000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552384570,"objectID":"78803cb3dcb1dcd228d9ba901813564f","permalink":"https://konstmish.github.io/post/19_neurips_uai/","publishdate":"2019-03-12T00:00:00+03:00","relpermalink":"/post/19_neurips_uai/","section":"post","summary":"After a successful round of reviews for ICML I was invited to serve on committee for two more important ML conferences.\n","tags":[],"title":"I am a member of the program committee for NeurIPS and UAI 2019","type":"post"},{"authors":null,"categories":null,"content":"There is no specific project to work on yet, but my research interests are almost a subset of what Martin and his group have worked on. I would not be surprised if it leads to more than one project and hope that this visit will not be the last one :).\nAs of today, the group\u0026rsquo;s website is available at https://mlo.epfl.ch/\n","date":1550350800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550436816,"objectID":"02e28fadc0ea78fca9e59f1ce2a79af8","permalink":"https://konstmish.github.io/post/18_epfl/","publishdate":"2019-02-17T00:00:00+03:00","relpermalink":"/post/18_epfl/","section":"post","summary":"I will be at EPFL, visiting the Machine Learning and Optimization Laboratory led by Martin Jaggi.\n","tags":[],"title":"I'm visiting Martin Jaggi from 18 February to 15 March","type":"post"},{"authors":null,"categories":null,"content":"   ","date":1549486800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550436816,"objectID":"d5b718e97528a7fe7f271e8406d8821d","permalink":"https://konstmish.github.io/post/18_group/","publishdate":"2019-02-07T00:00:00+03:00","relpermalink":"/post/18_group/","section":"post","summary":"Excluding visitors, we have 6 PhD students and 3 postdocs. The photo is taken just a couple of days ago at KAUST.\n","tags":[],"title":"Our group photo","type":"post"},{"authors":["Konstantin Mishchenko","Filip Hanzely","Peter Richtárik"],"categories":[],"content":"","date":1548709200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597756299,"objectID":"fbe92e3e7a44b95d7d8420f097e4f250","permalink":"https://konstmish.github.io/publication/19_99/","publishdate":"2019-01-29T00:00:00+03:00","relpermalink":"/publication/19_99/","section":"publication","summary":"In this paper we discuss sparsification of worker-to-server communication in large distributed systems. We improve upon algorithms that fit the following template: a local gradient estimate is computed independently by each worker, then communicated to a master, which subsequently performs averaging. The average is broadcast back to the workers, which use it to perform a gradient-type step to update the local version of the model. We observe that the above template is fundamentally inefficient in that too much data is unnecessarily communicated from the workers to the server, which slows down the overall system. We propose a fix based on a new update-sparsification method we develop in this work, which we suggest being used on top of existing methods. Namely, we develop a new variant of parallel block coordinate descent based on independent sparsification of the local gradient estimates before communication. We demonstrate that with only m/n blocks sent by each of n workers, where m is the total number of parameter blocks, the theoretical iteration complexity of the underlying distributed methods is essentially unaffected. As an illustration, this means that when n = 100 parallel workers are used, the communication of 99% blocks is redundant, and hence a waste of time. Our theoretical claims are supported through extensive numerical experiments which demonstrate an almost perfect match with our theory on a number of synthetic and real datasets.","tags":[],"title":"99% of Worker-Master Communication in Distributed Optimization Is Not Needed","type":"publication"},{"authors":["Konstantin Mishchenko","Eduard Gorbunov","Martin Takáč","Peter Richtárik"],"categories":[],"content":"","date":1548709200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568744518,"objectID":"143dc39dc8bb2e0b6d22795b175c2646","permalink":"https://konstmish.github.io/publication/19_diana/","publishdate":"2019-01-29T00:00:00+03:00","relpermalink":"/publication/19_diana/","section":"publication","summary":"Training very large machine learning models requires a distributed computing approach, with communication of the model updates often being the bottleneck. For this reason, several methods based on the compression (e.g., sparsification and/or quantization) of the updates were recently proposed, including QSGD (Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods are able to learn the gradients, which means that they necessarily suffer from several issues, such as the inability to converge to the true optimum in the batch mode, inability to work with a nonsmooth regularizer, and slow convergence rates. In this work we propose a new distributed learning method---DIANA---which resolves these issues via compression of gradient differences. We perform a theoretical analysis in the strongly convex and nonconvex settings and show that our rates are vastly superior to existing rates. Our analysis of block-quantization and differences between ℓ2 and ℓ∞ quantization closes the gaps in theory and practice. Finally, by applying our analysis technique to TernGrad, we establish the first convergence rate for this method.","tags":[],"title":"Distributed Learning with Compressed Gradient Differences","type":"publication"},{"authors":["Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1540933200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568744518,"objectID":"0a7a429d44102c5824cb9df1a70a73c9","permalink":"https://konstmish.github.io/publication/18_penalty/","publishdate":"2018-10-31T00:00:00+03:00","relpermalink":"/publication/18_penalty/","section":"publication","summary":"The last decade witnessed a rise in the importance of supervised  learning applications involving big data and big models.  Big data refers to  situations where the amounts of training data  available and needed causes difficulties in the training phase of the pipeline. Big model refers to  situations where large dimensional  and over-parameterized models are  needed for the application at hand. Both of these phenomena lead to a dramatic increase in research activity aimed at taming the issues via the design of new sophisticated optimization algorithms.  In this paper we turn attention to the big constraints scenario and argue that elaborate machine learning systems of the future will necessarily need to account for a large number of  real-world constraints, which will need to be incorporated  in the training process. This line of work is largely unexplored, and provides ample opportunities for future work and applications.  To handle the big constraints regime, we propose a stochastic penalty formulation which reduces the problem to the well understood big data regime. Our formulation has many interesting  properties which relate it  to the original problem in various ways, with  mathematical guarantees. We give a number of results specialized to nonconvex loss functions, smooth convex functions, strongly convex functions and convex constraints. We show through experiments that our approach can beat  competing approaches by several orders of magnitude when a medium accuracy solution is required.","tags":[],"title":"A Stochastic Penalty Model for Convex and Nonconvex Optimization with Big Constraints","type":"publication"},{"authors":null,"categories":null,"content":"This is my first time being a reviewer for such a venue, so I hope I will not be \u0026ldquo;that stupid reviewer\u0026rdquo; :)\n","date":1540501200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"1a5fa7d2efca3afdfcb504d0a3962a9b","permalink":"https://konstmish.github.io/post/19_icml/","publishdate":"2018-10-26T00:00:00+03:00","relpermalink":"/post/19_icml/","section":"post","summary":"I was asked to be a reviewer at ICML next year, and I was happy to accept.\n","tags":[],"title":"I am a program committee member for ICML 2019!","type":"post"},{"authors":null,"categories":null,"content":"In a 24 hours programming competition Alibek and I were solving tough mathematical and programming problems for the sake of fun and, maybe, some prizes. Fortunately, we managed to make it to top 100, which means that we are eligible for prizes from IEEE!\nLast year I participated in the same competition with Samuel Horvath, but back then we were only 123rd.\nLink to the results: https://ieeextreme.org/wp-content/uploads/2018/11/IEEEXtreme-12.0-Global-Ranking.pdf\n","date":1540069200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"6c7445a4cd73d95fc7fed85a49b45d92","permalink":"https://konstmish.github.io/post/18_ieeextreme/","publishdate":"2018-10-21T00:00:00+03:00","relpermalink":"/post/18_ieeextreme/","section":"post","summary":"On 20 October Alibek Sailanbayev and I got ranked 71st out of 4000 teams worldwide in the IEEEXtreme competition. ","tags":[],"title":"Prize winner at IEEEXtreme","type":"post"},{"authors":["Filip Hanzely","Konstantin Mishchenko","Peter Richtarik"],"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568744518,"objectID":"89bf505d1fd8e1a08723243a6d8e28c5","permalink":"https://konstmish.github.io/publication/18_sega/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/publication/18_sega/","section":"publication","summary":"We propose a randomized first order optimization method--SEGA (SkEtched GrAdient method)-- which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient obtained from an oracle. In each iteration, SEGA updates the current estimate of the gradient through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent.","tags":null,"title":"SEGA: Variance Reduction via Gradient Sketching","type":"publication"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m happy to say that our paper \u0026ldquo;SEGA: Variance Reduction via Gradient Sketching\u0026rdquo; about variance reduction for coordinate descent methods was accepted to NIPS. In this work, we discover how coordinate descent methods can be extended to problems with non-separable regularizer. We consider minibatch, importance sampling and acceleration of the produced method. What\u0026rsquo;s more, we additionally introduced a variant with non-trivial sketches of the gradient, under maximal possible stepsizes can be larger by an arbitrary number than in classical coordinate descent methods.\n","date":1535576400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"ed3eacdacd875091494e7f4f852eb55b","permalink":"https://konstmish.github.io/post/18_nips/","publishdate":"2018-08-30T00:00:00+03:00","relpermalink":"/post/18_nips/","section":"post","summary":"Our paper with Filip Hanzely and Peter Richtarik was accepted to the NIPS conference.\n","tags":[],"title":"A NIPS paper got accepted!","type":"post"},{"authors":["Konstantin Mishchenko","Franck Iutzeler","Jérôme Malick","Massih-Reza Amini"],"categories":null,"content":"","date":1531170000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569682811,"objectID":"87fe99fb553fa101a0e3da563e09a5e9","permalink":"https://konstmish.github.io/publication/18_dave_icml/","publishdate":"2018-07-10T00:00:00+03:00","relpermalink":"/publication/18_dave_icml/","section":"publication","summary":"Distributed learning aims at computing high-quality models by training over scattered data. This covers a diversity of scenarios, including computer clusters or mobile agents. One of the main challenges is then to deal with heterogeneous machines and unreliable communications. In this setting, we propose and analyze a flexible asynchronous optimization algorithm for solving nonsmooth learning problems. Unlike most existing methods, our algorithm is adjustable to various levels of communication costs, machines computational powers, and data distribution evenness. We prove that the algorithm converges linearly with a fixed learning rate that does not depend on communication delays nor on the number of machines. Although long delays in communication may slow down performance, no delay can break convergence.","tags":null,"title":"A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning","type":"publication"},{"authors":["Konstantin Mishchenko","Franck Iutzeler","Jérôme Malick"],"categories":null,"content":"","date":1529884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584886582,"objectID":"48b35bd35670fd5b8d502bcaeb25664a","permalink":"https://konstmish.github.io/publication/18_dave_siopt/","publishdate":"2018-06-25T00:00:00Z","relpermalink":"/publication/18_dave_siopt/","section":"publication","summary":"We develop and analyze an asynchronous algorithm for distributed convex optimization when the objective writes a sum of smooth functions, local to each worker, and a non-smooth function. Unlike many existing methods, our distributed algorithm is adjustable to various levels of communication cost, delays, machines computational power, and functions smoothness. A unique feature is that the stepsizes do not depend on communication delays nor number of machines, which is highly desirable for scalability. We prove that the algorithm converges linearly in the strongly convex case, and provide guarantees of convergence for the non-strongly convex case. The obtained rates are the same as the vanilla proximal gradient algorithm over some introduced epoch sequence that subsumes the delays of the system. We provide numerical results on large-scale machine learning problems to demonstrate the merits of the proposed method.","tags":null,"title":"A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm","type":"publication"},{"authors":null,"categories":null,"content":"During my MSc in Paris, we had an amazing course taught by Fabian Pedregosa (http://fa.bianp.net/) and Fajwel Fogel (http://www.di.ens.fr/~fogel/). In this course, we learnt how to use Python for data analysis and to evaluate our knowledge we needed to participate in a challenge with other Paris students. Together with Matthieu Mazzolini and Paul Dufosse (they are very cool guys) I chose PlumeLabs challenge where the data was the air pollution in different cities over time. Long story short, we won it, which is totally due to my teammates, I would never be able to do it alone.\nBack then, both Fabian and Fajwel were supervised by Alexandre d\u0026rsquo;Aspremont (postdoc and PhD student respectively). And they invited another postdoc of Alexandre, Federico Vaggi, to help them with grading our final presentations. Based on our grade, I guess that we made good impression on them, and I was pretty happy about it.\nWhat is interesting is that Federico just a few weeks later was already working at Amazon. Last Fall I wrote Federico an email and after a few interviews a got accepted to intern in the same research team. The topic of my internship is not specified yet, but it will involve research in machine learning for problems motivated by Amazon\u0026rsquo;s needs. The coolest thing is that while the team\u0026rsquo;s work is very applied, for my internship they agree to work on quite theoretical topics, so that I would be able to use it for my PhD. Let us see what results I will have by the end of my internship.\n","date":1524085200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"6392823bf9e74a75b5d3dc54fb3d83ba","permalink":"https://konstmish.github.io/post/18_amazon/","publishdate":"2018-04-19T00:00:00+03:00","relpermalink":"/post/18_amazon/","section":"post","summary":"From 4 June to 28 September, Seattle.\n","tags":[],"title":"I'm interning at Amazon August-November","type":"post"},{"authors":null,"categories":null,"content":"To begin with, the pope didn\u0026rsquo;t come to us to say \u0026ldquo;hi\u0026rdquo;. He gave a speech from a balcony instead and it was the first time he said \u0026ldquo;hackathon\u0026rdquo; in public. Moreover, it would be unfair to say that we were not welcome: the Vatican invested deeply in this event.\nThe hackathon went well, although our team didn\u0026rsquo;t win anything. It was a nice experience and since I didn\u0026rsquo;t expect to win a hackathon at the first attempt, I\u0026rsquo;m pretty happy about having being there. And as we actually did work a lot on our project, after coming back to KAUST I had an amazing 12-hours sleep.\nWhat was really surprising is the dinner that we had in a private palace. This palace is called \u0026ldquo;Palazzo Taverna\u0026rdquo; and was built by Cardinal Giordano Orsini approximately in 1400. There is a short wikipedia page in Italian about it: link. Every room looks so fancy, there are so many paintings everywhere, it looks overwhelmingly luxurious. I bet everyone was impressed.\n","date":1521493200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"1d6d0d1ba594d15fe5df6d958acfb247","permalink":"https://konstmish.github.io/post/18mar_vatican/","publishdate":"2018-03-20T00:00:00+03:00","relpermalink":"/post/18mar_vatican/","section":"post","summary":"I came back after the hackathon organized by the Vatican. Inside a few pictures and my impression.\n","tags":[],"title":"After the first hackathon at the Vatican","type":"post"},{"authors":null,"categories":null,"content":"I was selected to be one of 120 students invited to the Vatican to participate in a hackathon. Hackathon\u0026rsquo;s theme are (1) social inclusion, (2) interfaith dialogue and (3) migrants \u0026amp; refugees. I am no expert in religions or migrants problems, so I applied and got accepted to participate addressing the first theme.\nThe even is organized by OPTIC - \u0026ldquo;an organization founded in 2012 dedicated to research and innovation regarding the ethics of new and disruptive technologies\u0026rdquo;. More details can be found on this webpage: https://vhacks.org/faq/\nI don\u0026rsquo;t have a good hackathon participations record, but I believe it will be amazing :) It is especially nice to represent KAUST in such an event.\n","date":1516309200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"c405b50eb6394044c3156d9cddcdebcb","permalink":"https://konstmish.github.io/post/18jan_vatican/","publishdate":"2018-01-19T00:00:00+03:00","relpermalink":"/post/18jan_vatican/","section":"post","summary":"This March I'm going to the Vatican to participate in a 3 days hackathon.\n","tags":[],"title":"VHacks – A hackathon at the Vatican","type":"post"},{"authors":null,"categories":null,"content":"In September Peter suggested me to take part in organizing a few sessions at INFORMS Optimization Society Conference. Since I had no idea of what it means back then, I decided to first try with just one. In addition, I could choose to invite either 3 or 4 speakers, and I decided to go for the former option. It allowed me to choose speakers carefully, but unfortunately I didn\u0026rsquo;t foresee that one of the speakers I invited may not be able to come. So now I am going to be a speaker there as well, which is a common phenomenon, though I rather wanted to listen than to present.\n","date":1513717200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"d86f2a90e242bd7d943eff48ee5d2b2c","permalink":"https://konstmish.github.io/post/18_informs/","publishdate":"2017-12-20T00:00:00+03:00","relpermalink":"/post/18_informs/","section":"post","summary":"I'm co-organizing a session with Peter Richtarik on Distributed Optimization and Large Scale Machine Learning.\n","tags":[],"title":"INFORMS Optimization Society Conference","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m visiting Dr. Carola-Bibiane Schönlieb\u0026rsquo;s group at Cambridge until 9 January. I will also be here for a few more weeks in February.\n","date":1513198800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"c96de649ada09f885a05b0a6c255d01b","permalink":"https://konstmish.github.io/post/17_visiting_cambridge/","publishdate":"2017-12-14T00:00:00+03:00","relpermalink":"/post/17_visiting_cambridge/","section":"post","summary":"I'm at Cambridge University until 9 January.\n","tags":[],"title":"Visiting Cambridge","type":"post"},{"authors":null,"categories":null,"content":"I teamed up with Samuel Horvath to participate in a competition called \u0026ldquo;IEEEXtreme\u0026rdquo; organized by IEEE for students worldwide. We got ranked 123 worldwide or 43 in our Region (Africa, Europe and Middle East). Unfortunately, we didn\u0026rsquo;t get in top 100, so we are not going to get T-shirts as a prize, but the competition was extremely fun and it was cool to work with Samuel in a team.\nOfficial results are available here: http://ieeextreme.org/files/2017/11/IEEEXtreme-11.0-Rankings-Globally.pdf\n","date":1508619600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565987553,"objectID":"449c592c92534e89e7b73ca272fee988","permalink":"https://konstmish.github.io/post/17_ieeextreme/","publishdate":"2017-10-22T00:00:00+03:00","relpermalink":"/post/17_ieeextreme/","section":"post","summary":"Samuel Horvath and me got 123 place out of 3350 teams.\n","tags":[],"title":"IEEEXtreme","type":"post"}]