[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a PhD student under the supervision of Peter Richtárik, working on Optimization and its applications to Machine Learning. Prior to this, I was a research intern at Université Grenoble Alpes, where I studied an algorithm for Distributed Optimization with F. Iutzeler and J. Malick. This internship allowed me to obtain my double degree MSc diploma from École Normale Supérieure Paris-Saclay and Paris-Dauphine. I obtained my BSc from Moscow Institute of Physics and Technology, where I was having a lot of fun with quantum mechanics and various branches of classical and modern physics. My bachelor\u0026rsquo;s thesis, however, was devoted to statistical learning theory.\nMy hobbies include squash, bouldering and competitive programming.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://konstmish.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I\u0026rsquo;m a PhD student under the supervision of Peter Richtárik, working on Optimization and its applications to Machine Learning. Prior to this, I was a research intern at Université Grenoble Alpes, where I studied an algorithm for Distributed Optimization with F. Iutzeler and J. Malick. This internship allowed me to obtain my double degree MSc diploma from École Normale Supérieure Paris-Saclay and Paris-Dauphine. I obtained my BSc from Moscow Institute of Physics and Technology, where I was having a lot of fun with quantum mechanics and various branches of classical and modern physics.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://konstmish.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"   ","date":1549486800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549486800,"objectID":"e9967b4d7a16ce90f53452ec9abf1c99","permalink":"https://konstmish.github.io/post/2018_group/","publishdate":"2019-02-07T00:00:00+03:00","relpermalink":"/post/2018_group/","section":"post","summary":"Excluding visitors, we have 6 PhD students and 3 postdocs. The photo is taken just a couple of days ago at KAUST.\n","tags":[],"title":"Our group photo","type":"post"},{"authors":["Konstantin Mishchenko","Filip Hanzely","Peter Richtárik"],"categories":[],"content":"","date":1548709200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548709200,"objectID":"edb6f30d125cf0051aecb87b38c90436","permalink":"https://konstmish.github.io/publication/99/","publishdate":"2019-01-29T00:00:00+03:00","relpermalink":"/publication/99/","section":"publication","summary":"It is well known that many optimization methods, including SGD, SAGA, and Accelerated SGD for over-parameterized models, do not scale linearly in the parallel setting. In this paper, we present a new version of block coordinate descent that solves this issue for a number of methods. The core idea is to make the sampling of coordinate blocks on each parallel unit independent of the others. Surprisingly, we prove that the optimal number of blocks to be updated by each of n units in every iteration is equal to m/n, where m is the total number of blocks. As an illustration, this means that when n=100 parallel units are used, 99% of work is a waste of time. We demonstrate that with m/n blocks used by each unit the iteration complexity often remains the same. Among other applications which we mention, this fact can be exploited in the setting of distributed optimization to break the communication bottleneck. Our claims are justified by numerical experiments which demonstrate almost a perfect match with our theory on a number of datasets.","tags":[],"title":"99% of Parallel Optimization is Inevitably a Waste of Time","type":"publication"},{"authors":["Konstantin Mishchenko","Eduard Gorbunov","Martin Takáč","Peter Richtárik"],"categories":[],"content":"","date":1548709200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548709200,"objectID":"23140cc8894f9ff9e3c0bd96762df738","permalink":"https://konstmish.github.io/publication/diana/","publishdate":"2019-01-29T00:00:00+03:00","relpermalink":"/publication/diana/","section":"publication","summary":"Training very large machine learning models requires a distributed computing approach, with communication of the model updates often being the bottleneck. For this reason, several methods based on the compression (e.g., sparsification and/or quantization) of the updates were recently proposed, including QSGD (Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et al., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods are able to learn the gradients, which means that they necessarily suffer from several issues, such as the inability to converge to the true optimum in the batch mode, inability to work with a nonsmooth regularizer, and slow convergence rates. In this work we propose a new distributed learning method---DIANA---which resolves these issues via compression of gradient differences. We perform a theoretical analysis in the strongly convex and nonconvex settings and show that our rates are vastly superior to existing rates. Our analysis of block-quantization and differences between ℓ2 and ℓ∞ quantization closes the gaps in theory and practice. Finally, by applying our analysis technique to TernGrad, we establish the first convergence rate for this method.","tags":[],"title":"Distributed Learning with Compressed Gradient Differences","type":"publication"},{"authors":["Konstantin Mishchenko","Peter Richtárik"],"categories":[],"content":"","date":1540933200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540933200,"objectID":"24b7526335c552d615af8e69f1bc00be","permalink":"https://konstmish.github.io/publication/penalty/","publishdate":"2018-10-31T00:00:00+03:00","relpermalink":"/publication/penalty/","section":"publication","summary":"The last decade witnessed a rise in the importance of supervised  learning applications involving big data and big models.  Big data refers to  situations where the amounts of training data  available and needed causes difficulties in the training phase of the pipeline. Big model refers to  situations where large dimensional  and over-parameterized models are  needed for the application at hand. Both of these phenomena lead to a dramatic increase in research activity aimed at taming the issues via the design of new sophisticated optimization algorithms.  In this paper we turn attention to the big constraints scenario and argue that elaborate machine learning systems of the future will necessarily need to account for a large number of  real-world constraints, which will need to be incorporated  in the training process. This line of work is largely unexplored, and provides ample opportunities for future work and applications.  To handle the big constraints regime, we propose a stochastic penalty formulation which reduces the problem to the well understood big data regime. Our formulation has many interesting  properties which relate it  to the original problem in various ways, with  mathematical guarantees. We give a number of results specialized to nonconvex loss functions, smooth convex functions, strongly convex functions and convex constraints. We show through experiments that our approach can beat  competing approaches by several orders of magnitude when a medium accuracy solution is required.","tags":[],"title":"A Stochastic Penalty Model for Convex and Nonconvex Optimization with Big Constraints","type":"publication"},{"authors":null,"categories":null,"content":"This is my first time being a reviewer for such a venue, so I hope I will not be \u0026ldquo;that stupid reviewer\u0026rdquo; :)\n","date":1540501200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540501200,"objectID":"410c58c138f8cd6f2e2e83a2cf5b4be4","permalink":"https://konstmish.github.io/post/icml-2019/","publishdate":"2018-10-26T00:00:00+03:00","relpermalink":"/post/icml-2019/","section":"post","summary":"I was asked to be a reviewer at ICML next year, and I was happy to accept.\n","tags":[],"title":"I am a program committee member for ICML 2019!","type":"post"},{"authors":null,"categories":null,"content":"In a 24 hours programming competition Alibek and I were solving tough mathematical and programming problems for the sake of fun and, maybe, some prizes. Fortunately, we managed to make it to top 100, which means that we are eligible for prizes from IEEE!\nLast year I participated in the same competition with Samuel Horvath, but back then we were only 123rd.\nLink to the results: https://ieeextreme.org/wp-content/uploads/2018/11/IEEEXtreme-12.0-Global-Ranking.pdf\n","date":1540069200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540069200,"objectID":"65d1dce325b003402532d344d7d6bc8f","permalink":"https://konstmish.github.io/post/ieeextreme-2018/","publishdate":"2018-10-21T00:00:00+03:00","relpermalink":"/post/ieeextreme-2018/","section":"post","summary":"On 20 October Alibek Sailanbayev and I got ranked 71st out of 4000 teams worldwide in the IEEEXtreme competition. ","tags":[],"title":"Prize winner at IEEEXtreme","type":"post"},{"authors":["Filip Hanzely","Konstantin Mishchenko","Peter Richtarik"],"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"1c4394892d60f89713e52b0b68137e20","permalink":"https://konstmish.github.io/publication/sega/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/publication/sega/","section":"publication","summary":"We propose a randomized first order optimization method--SEGA (SkEtched GrAdient method)-- which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient obtained from an oracle. In each iteration, SEGA updates the current estimate of the gradient through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent.","tags":null,"title":"SEGA: Variance Reduction via Gradient Sketching","type":"publication"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m happy to say that our paper \u0026ldquo;SEGA: Variance Reduction via Gradient Sketching\u0026rdquo; about variance reduction for coordinate descent methods was accepted to NIPS. In this work, we discover how coordinate descent methods can be extended to problems with non-separable regularizer. We consider minibatch, importance sampling and acceleration of the produced method. What\u0026rsquo;s more, we additionally introduced a variant with non-trivial sketches of the gradient, under maximal possible stepsizes can be larger by an arbitrary number than in classical coordinate descent methods.\n","date":1535576400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535576400,"objectID":"d040e5e0b2fb10a791e4e2b1f2eb14d3","permalink":"https://konstmish.github.io/post/nips-2018/","publishdate":"2018-08-30T00:00:00+03:00","relpermalink":"/post/nips-2018/","section":"post","summary":"Our paper with Filip Hanzely and Peter Richtarik was accepted to the NIPS conference.\n","tags":[],"title":"A NIPS paper got accepted!","type":"post"},{"authors":["Konstantin Mishchenko","Franck Iutzeler","Jérôme Malick"],"categories":null,"content":"","date":1529884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529884800,"objectID":"c54183f8c6514dd363b1a52e782810c8","permalink":"https://konstmish.github.io/publication/dave_siopt/","publishdate":"2018-06-25T00:00:00Z","relpermalink":"/publication/dave_siopt/","section":"publication","summary":"We develop and analyze an asynchronous algorithm for distributed convex optimization when the objective writes a sum of smooth functions, local to each worker, and a non-smooth function. Unlike many existing methods, our distributed algorithm is adjustable to various levels of communication cost, delays, machines computational power, and functions smoothness. A unique feature is that the stepsizes do not depend on communication delays nor number of machines, which is highly desirable for scalability. We prove that the algorithm converges linearly in the strongly convex case, and provide guarantees of convergence for the non-strongly convex case. The obtained rates are the same as the vanilla proximal gradient algorithm over some introduced epoch sequence that subsumes the delays of the system. We provide numerical results on large-scale machine learning problems to demonstrate the merits of the proposed method.","tags":null,"title":"A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm","type":"publication"},{"authors":null,"categories":null,"content":"During my MSc in Paris, we had an amazing course taught by Fabian Pedregosa (http://fa.bianp.net/) and Fajwel Fogel (http://www.di.ens.fr/~fogel/). In this course, we learnt how to use Python for data analysis and to evaluate our knowledge we needed to participate in a challenge with other Paris students. Together with Matthieu Mazzolini and Paul Dufosse (they are very cool guys) I chose PlumeLabs challenge where the data was the air pollution in different cities over time. Long story short, we won it, which is totally due to my teammates, I would never be able to do it alone.\nBack then, both Fabian and Fajwel were supervised by Alexandre d\u0026rsquo;Aspremont (postdoc and PhD student respectively). And they invited another postdoc of Alexandre, Federico Vaggi, to help them with grading our final presentations. Based on our grade, I guess that we made good impression on them, and I was pretty happy about it.\nWhat is interesting is that Federico just a few weeks later was already working at Amazon. Last Fall I wrote Federico an email and after a few interviews a got accepted to intern in the same research team. The topic of my internship is not specified yet, but it will involve research in machine learning for problems motivated by Amazon\u0026rsquo;s needs. The coolest thing is that while the team\u0026rsquo;s work is very applied, for my internship they agree to work on quite theoretical topics, so that I would be able to use it for my PhD. Let us see what results I will have by the end of my internship.\n","date":1524085200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524085200,"objectID":"6dc1fc7949499493ca06b31d45581590","permalink":"https://konstmish.github.io/post/amazon-2018/","publishdate":"2018-04-19T00:00:00+03:00","relpermalink":"/post/amazon-2018/","section":"post","summary":"From 4 June to 28 September, Seattle.\n","tags":[],"title":"I'm interning at Amazon August-November","type":"post"},{"authors":null,"categories":null,"content":"To begin with, the pope didn\u0026rsquo;t come to us to say \u0026ldquo;hi\u0026rdquo;. He gave a speech from a balcony instead and it was the first time he said \u0026ldquo;hackathon\u0026rdquo; in public. Moreover, it would be unfair to say that we were not welcome: the Vatican invested deeply in this event.\nThe hackathon went well, although our team didn\u0026rsquo;t win anything. It was a nice experience and since I didn\u0026rsquo;t expect to win a hackathon at the first attempt, I\u0026rsquo;m pretty happy about having being there. And as we actually did work a lot on our project, after coming back to KAUST I had an amazing 12-hours sleep.\nWhat was really surprising is the dinner that we had in a private palace. This palace is called \u0026ldquo;Palazzo Taverna\u0026rdquo; and was built by Cardinal Giordano Orsini approximately in 1400. There is a short wikipedia page in Italian about it: link. Every room looks so fancy, there are so many paintings everywhere, it looks overwhelmingly luxurious. I bet everyone was impressed.\n","date":1521493200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521493200,"objectID":"5f71c8738b59df39b7f00a59afca6628","permalink":"https://konstmish.github.io/post/vatican-2018mar/","publishdate":"2018-03-20T00:00:00+03:00","relpermalink":"/post/vatican-2018mar/","section":"post","summary":"I came back after the hackathon organized by the Vatican. Inside a few pictures and my impression.\n","tags":[],"title":"After the first hackathon at the Vatican","type":"post"},{"authors":null,"categories":null,"content":"I was selected to be one of 120 students invited to the Vatican to participate in a hackathon. Hackathon\u0026rsquo;s theme are (1) social inclusion, (2) interfaith dialogue and (3) migrants \u0026amp; refugees. I am no expert in religions or migrants problems, so I applied and got accepted to participate addressing the first theme.\nThe even is organized by OPTIC - \u0026ldquo;an organization founded in 2012 dedicated to research and innovation regarding the ethics of new and disruptive technologies\u0026rdquo;. More details can be found on this webpage: https://vhacks.org/faq/\nI don\u0026rsquo;t have a good hackathon participations record, but I believe it will be amazing :) It is especially nice to represent KAUST in such an event.\n","date":1516309200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516309200,"objectID":"ae017f15f7ba9a91435215ea5cdfd99c","permalink":"https://konstmish.github.io/post/vatican-2018jan/","publishdate":"2018-01-19T00:00:00+03:00","relpermalink":"/post/vatican-2018jan/","section":"post","summary":"This March I'm going to the Vatican to participate in a 3 days hackathon.\n","tags":[],"title":"VHacks – A hackathon at the Vatican","type":"post"},{"authors":null,"categories":null,"content":"In September Peter suggested me to take part in organizing a few sessions at INFORMS Optimization Society Conference. Since I had no idea of what it means back then, I decided to first try with just one. In addition, I could choose to invite either 3 or 4 speakers, and I decided to go for the former option. It allowed me to choose speakers carefully, but unfortunately I didn\u0026rsquo;t foresee that one of the speakers I invited may not be able to come. So now I am going to be a speaker there as well, which is a common phenomenon, though I rather wanted to listen than to present.\n","date":1513717200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513717200,"objectID":"70e6b91367e77f05d2968e20613cdc10","permalink":"https://konstmish.github.io/post/informs-2018/","publishdate":"2017-12-20T00:00:00+03:00","relpermalink":"/post/informs-2018/","section":"post","summary":"I'm co-organizing a session with Peter Richtarik on Distributed Optimization and Large Scale Machine Learning.\n","tags":[],"title":"INFORMS Optimization Society Conference","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m visiting Dr. Carola-Bibiane Schönlieb\u0026rsquo;s group at Cambridge until 9 January. I will also be here for a few more weeks in February.\n","date":1513198800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513198800,"objectID":"39fe9aee9432a5e28203537de1b9d860","permalink":"https://konstmish.github.io/post/visiting-cambridge-2017/","publishdate":"2017-12-14T00:00:00+03:00","relpermalink":"/post/visiting-cambridge-2017/","section":"post","summary":"I'm at Cambridge University until 9 January.\n","tags":[],"title":"Visiting Cambridge","type":"post"},{"authors":null,"categories":null,"content":"I teamed up with Samuel Horvath to participate in a competition called \u0026ldquo;IEEEXtreme\u0026rdquo; organized by IEEE for students worldwide. We got ranked 123 worldwide or 43 in our Region (Africa, Europe and Middle East). Unfortunately, we didn\u0026rsquo;t get in top 100, so we are not going to get T-shirts as a prize, but the competition was extremely fun and it was cool to work with Samuel in a team.\nOfficial results are available here: http://ieeextreme.org/files/2017/11/IEEEXtreme-11.0-Rankings-Globally.pdf\n","date":1508619600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508619600,"objectID":"b077ca9b0cfffcd595d84900d249011b","permalink":"https://konstmish.github.io/post/ieeextreme/","publishdate":"2017-10-22T00:00:00+03:00","relpermalink":"/post/ieeextreme/","section":"post","summary":"Samuel Horvath and me got 123 place out of 3350 teams.\n","tags":[],"title":"IEEEXtreme","type":"post"},{"authors":["Konstantin Mishchenko","Franck Iutzeler","Jérôme Malick","Massih-Reza Amini"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a08d55edcf0570f14f5039599cf03b0b","permalink":"https://konstmish.github.io/publication/dave_icml/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/dave_icml/","section":"publication","summary":"Distributed learning aims at computing high-quality models by training over scattered data. This covers a diversity of scenarios, including computer clusters or mobile agents. One of the main challenges is then to deal with heterogeneous machines and unreliable communications. In this setting, we propose and analyze a flexible asynchronous optimization algorithm for solving nonsmooth learning problems. Unlike most existing methods, our algorithm is adjustable to various levels of communication costs, machines computational powers, and data distribution evenness. We prove that the algorithm converges linearly with a fixed learning rate that does not depend on communication delays nor on the number of machines. Although long delays in communication may slow down performance, no delay can break convergence.","tags":null,"title":"A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning","type":"publication"}]